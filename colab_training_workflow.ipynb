{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进阶虛擬貨幣價格預測訓練\n",
    "## Colab完整工作流程\n",
    "\n",
    "本筆記本自動执行以下步驟:\n",
    "1. 環境設定 \u2328 GPU优化",
    "2. 依賴套件安裝",
    "3. HuggingFace數據自勘下載",
    "4. LSTM模型訓練",
    "5. 模型上傳HF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第一步：Colab環境設定 ========================\n",
    "print(\"[1/7] Colab環境設定...\")",
    "\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# 棄保",
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "    print(\"  ✔ Google Colab環境偵測成功\")\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "    print(\"  ⚠ 本地環境模式\")\n",
    "\n",
    "# GPU配置\n",
    "print(\"  GPU优化配置...\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"  ✔ 偵測到 {len(gpus)} 個GPU\")\n",
    "    else:\n",
    "        print(\"  ⚠ 未偵測到GPU\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ GPU配置警告: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第二步：安裝依賴套件 ========================\n",
    "print(\"\\n[2/7] 安裝依賴套件...\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "packages = {\n",
    "    'tensorflow': 'TensorFlow',\n",
    "    'keras': 'Keras',\n",
    "    'huggingface_hub': 'Hugging Face Hub',\n",
    "    'pandas': 'Pandas',\n",
    "    'numpy': 'NumPy',\n",
    "    'sklearn': 'Scikit-Learn',\n",
    "    'requests': 'Requests'\n",
    "}\n",
    "\n",
    "for module, name in packages.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"  ✔ {name} 已安裝\")\n",
    "    except ImportError:\n",
    "        print(f\"  安裝 {name}...\")\n",
    "        os.system(f'pip install -q {module.replace(\"_\", \"-\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第三步：定義技術指標 ========================\n",
    "print(\"\\n[3/7] 技術指標計算...\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:period+1]\n",
    "    up = seed[seed >= 0].sum() / period\n",
    "    down = -seed[seed < 0].sum() / period\n",
    "    rs = up / down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:period] = 100. - 100. / (1. + rs)\n",
    "    \n",
    "    for i in range(period, len(prices)):\n",
    "        delta = deltas[i-1]\n",
    "        if delta > 0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "        \n",
    "        up = (up * (period - 1) + upval) / period\n",
    "        down = (down * (period - 1) + downval) / period\n",
    "        rs = up / down\n",
    "        rsi[i] = 100. - 100. / (1. + rs)\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    ema_fast = pd.Series(prices).ewm(span=fast).mean().values\n",
    "    ema_slow = pd.Series(prices).ewm(span=slow).mean().values\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = pd.Series(macd).ewm(span=signal).mean().values\n",
    "    histogram = macd - signal_line\n",
    "    return macd, signal_line, histogram\n",
    "\n",
    "def calculate_bollinger_bands(prices, period=20, num_std=2):\n",
    "    sma = pd.Series(prices).rolling(window=period).mean().values\n",
    "    std = pd.Series(prices).rolling(window=period).std().values\n",
    "    upper_band = sma + (std * num_std)\n",
    "    lower_band = sma - (std * num_std)\n",
    "    return upper_band, sma, lower_band\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    close_prices = df['close'].values\n",
    "    df['rsi'] = calculate_rsi(close_prices, period=14)\n",
    "    df['macd'], df['macd_signal'], df['macd_hist'] = calculate_macd(close_prices)\n",
    "    upper_bb, mid_bb, lower_bb = calculate_bollinger_bands(close_prices)\n",
    "    df['bb_position'] = np.where(\n",
    "        upper_bb == lower_bb,\n",
    "        0.5,\n",
    "        (close_prices - lower_bb) / (upper_bb - lower_bb)\n",
    "    )\n",
    "    df = df.fillna(method='bfill').fillna(method='ffill')\n",
    "    return df\n",
    "\n",
    "print(\"  ✔ 技術指標凖讀成功\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第四步：从 HF 自勘下載数据 ========================\n",
    "print(\"\\n[4/7] 数据下載扣調...\")\n",
    "\n",
    "os.makedirs(\"./data/klines_binance_us\", exist_ok=True)\n",
    "os.makedirs(\"./all_models\", exist_ok=True)\n",
    "\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "\n",
    "DATASET_ID = \"zongowo111/cpb-models\"\n",
    "DATA_DIR = \"./data/klines_binance_us\"\n",
    "\n",
    "print(f\"  从 HF 下載: {DATASET_ID}\")\n",
    "\n",
    "try:\n",
    "    # 列出 HF 上的所有檔案\n",
    "    files = list_repo_files(repo_id=DATASET_ID, repo_type=\"dataset\", revision=\"main\")\n",
    "    \n",
    "    # 篩選 klines_binance_us 資料夾中的 CSV 檔案\n",
    "    csv_files = [f for f in files if f.startswith('klines_binance_us/') and f.endswith('.csv')]\n",
    "    \n",
    "    print(f\"  ✔ 找到 {len(csv_files)} 個 CSV 檔案\")\n",
    "    \n",
    "    # 建立符號資料夾\n",
    "    symbols = {}\n",
    "    for csv_file in csv_files:\n",
    "        parts = csv_file.split('/')\n",
    "        if len(parts) == 3:\n",
    "            symbol = parts[1]\n",
    "            if symbol not in symbols:\n",
    "                symbols[symbol] = []\n",
    "            symbols[symbol].append(csv_file)\n",
    "    \n",
    "    print(f\"  ✔ 找到 {len(symbols)} 個幣種\")\n",
    "    \n",
    "    # 下載檔案\n",
    "    downloaded_count = 0\n",
    "    pairs_to_train = []\n",
    "    \n",
    "    for symbol in sorted(symbols.keys()):\n",
    "        symbol_path = f\"{DATA_DIR}/{symbol}\"\n",
    "        os.makedirs(symbol_path, exist_ok=True)\n",
    "        \n",
    "        for csv_file in symbols[symbol]:\n",
    "            filename = csv_file.split('/')[-1]\n",
    "            local_path = f\"{symbol_path}/{filename}\"\n",
    "            \n",
    "            if not os.path.exists(local_path):\n",
    "                try:\n",
    "                    print(f\"    下載 {csv_file}...\", end=' ')\n",
    "                    hf_hub_download(\n",
    "                        repo_id=DATASET_ID,\n",
    "                        filename=csv_file,\n",
    "                        repo_type=\"dataset\",\n",
    "                        local_dir=\"./data\"\n",
    "                    )\n",
    "                    downloaded_count += 1\n",
    "                    print(\"✔\")\n",
    "                except Exception as e:\n",
    "                    print(f\"\\r✗ ({str(e)[:30]})\")\n",
    "            else:\n",
    "                downloaded_count += 1\n",
    "            \n",
    "            timeframe = filename.split('_')[1] if '_' in filename else 'unknown'\n",
    "            pairs_to_train.append((symbol, timeframe, local_path))\n",
    "    \n",
    "    print(f\"  ✔ 總共下載 {downloaded_count} 個檔案\")\n",
    "    print(f\"  ✔ 準備訓練 {len(pairs_to_train)} 個模型\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ✗ 下載失敗: {e}\")\n",
    "    print(\"  嘗試使用測試數据...\")\n",
    "    pairs_to_train = [(\"BTC\", \"15m\", None), (\"ETH\", \"15m\", None)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第五步：模型定義下載手 4連 上傳 ========================\n",
    "print(\"\\n[5/7] 模型訓練中...\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(lookback=60, future_steps=10):\n",
    "    model = Sequential([\n",
    "        LSTM(128, activation='relu', input_shape=(lookback, 9), return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, activation='relu', return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(future_steps * 4)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_sequences(df, lookback=60, future_steps=10):\n",
    "    data = df[['open', 'high', 'low', 'close', 'rsi', 'macd', 'macd_signal', 'macd_hist', 'bb_position']].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - lookback - future_steps + 1):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback:i+lookback+future_steps, :4].flatten())\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "trained_count = 0\n",
    "\n",
    "# 訓練前 20 個模型\n",
    "for idx, (symbol, timeframe, csv_path) in enumerate(pairs_to_train[:20], 1):\n",
    "    print(f\"\\n  [{idx}/{min(20, len(pairs_to_train))}] {symbol} {timeframe}\")\n",
    "    \n",
    "    try:\n",
    "        # 載入數据\n",
    "        if csv_path and os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df.columns = [col.lower().strip() for col in df.columns]\n",
    "            \n",
    "            required_cols = ['open', 'high', 'low', 'close']\n",
    "            if not all(col in df.columns for col in required_cols):\n",
    "                df = df.iloc[:, 1:]\n",
    "                df.columns = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "        else:\n",
    "            np.random.seed(42 + hash(symbol + timeframe) % 1000)\n",
    "            df = pd.DataFrame({\n",
    "                'open': np.random.uniform(0.9, 1.1, 10000).cumprod() * 100,\n",
    "                'high': np.random.uniform(1.0, 1.15, 10000).cumprod() * 100,\n",
    "                'low': np.random.uniform(0.85, 1.0, 10000).cumprod() * 100,\n",
    "                'close': np.random.uniform(0.9, 1.1, 10000).cumprod() * 100,\n",
    "                'volume': np.random.uniform(1000, 10000, 10000)\n",
    "            })\n",
    "        \n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        df = df.dropna()\n",
    "        \n",
    "        if len(df) < 200:\n",
    "            print(f\"    ⚠ 數据不足 ({len(df)} < 200)\")\n",
    "            continue\n",
    "        \n",
    "        df = add_technical_indicators(df)\n",
    "        df = df.iloc[30:].reset_index(drop=True)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        price_cols = ['open', 'high', 'low', 'close']\n",
    "        df[price_cols] = scaler.fit_transform(df[price_cols])\n",
    "        \n",
    "        X, y = prepare_sequences(df, lookback=60, future_steps=10)\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "        X_val, y_val = X[split_idx:], y[split_idx:]\n",
    "        \n",
    "        model = create_model(lookback=60, future_steps=10)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=25,\n",
    "            batch_size=16,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        os.makedirs(f\"./all_models/{symbol}\", exist_ok=True)\n",
    "        model_path = f\"./all_models/{symbol}/{symbol}_{timeframe}_v8.keras\"\n",
    "        model.save(model_path)\n",
    "        \n",
    "        trained_count += 1\n",
    "        loss = history.history['loss'][-1]\n",
    "        print(f\"    ✔ 訓練完成 - loss: {loss:.6f}\")\n",
    "        \n",
    "        del model, X, y, X_train, y_train, X_val, y_val, df\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ 訓練失敖: {str(e)[:60]}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n  ✔ 成功訓練 {trained_count} 個檔案\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第六步：上傳模型到 Hugging Face ========================\n",
    "print(\"\\n[6/7] 上傳模型到 Hugging Face...\")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "    \n",
    "    api = HfApi()\n",
    "    upload_count = 0\n",
    "    \n",
    "    for symbol in os.listdir(\"./all_models\"):\n",
    "        symbol_path = f\"./all_models/{symbol}\"\n",
    "        \n",
    "        if not os.path.isdir(symbol_path):\n",
    "            continue\n",
    "        \n",
    "        for model_file in os.listdir(symbol_path):\n",
    "            if model_file.endswith('.keras'):\n",
    "                local_path = os.path.join(symbol_path, model_file)\n",
    "                repo_path = f\"models_v8/{symbol}/{model_file}\"\n",
    "                \n",
    "                try:\n",
    "                    api.upload_file(\n",
    "                        path_or_fileobj=local_path,\n",
    "                        path_in_repo=repo_path,\n",
    "                        repo_id=DATASET_ID,\n",
    "                        repo_type=\"dataset\",\n",
    "                        commit_message=f\"Upload {symbol} {model_file}\"\n",
    "                    )\n",
    "                    upload_count += 1\n",
    "                    print(f\"  ✔ {repo_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠ {repo_path} - {str(e)[:40]}\")\n",
    "    \n",
    "    print(f\"  ✔ 成功上傳 {upload_count} 個檔案\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ✗ 上傳失敖: {e}\")\n",
    "    print(\"  提示: 請確保已設定 HF Token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 第七步：摘要並完成 ========================\n",
    "print(\"\\n[7/7] 完成\")\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"trained_models\": trained_count,\n",
    "    \"total_pairs\": len(pairs_to_train),\n",
    "    \"dataset\": DATASET_ID,\n",
    "    \"status\": \"completed\"\n",
    "}\n",
    "\n",
    "with open(\"./training_summary.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"="*60)\n",
    "print(\"訓練上傳完成\")\n",
    "print(f\"訓練模型: {trained_count}\")\n",
    "print(f\"準備模型: {len(pairs_to_train)}\")\n",
    "print(f\"細誤時間: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"="*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
